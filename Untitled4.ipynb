{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637269d8",
   "metadata": {},
   "source": [
    "# Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a42df4ee-1b52-4103-b090-c591e05df056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./micromamba/lib/python3.9/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./micromamba/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./micromamba/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./micromamba/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./micromamba/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./micromamba/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454f8363-739a-44fe-9ab3-b5ea3258adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a direct download link for the Google Drive file and read into a dataframe\n",
    "url = 'https://drive.google.com/uc?id=1L8AUVWoSToH5dPQeVKo-LaWTbiaAsjwa&export=download'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b5175ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Ship_Date</th>\n",
       "      <th>Ship_Mode</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>City</th>\n",
       "      <th>State/Province</th>\n",
       "      <th>Postal_Code</th>\n",
       "      <th>Division</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product_Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Units</th>\n",
       "      <th>Gross_Profit</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Factory</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>ship_delay_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>128055</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>94122</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Wonka Bar - Triple Dazzle Caramel</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2</td>\n",
       "      <td>4.90</td>\n",
       "      <td>2.60</td>\n",
       "      <td>Wicked Choccy's</td>\n",
       "      <td>32.076176</td>\n",
       "      <td>-81.088371</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>128055</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>94122</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Wonka Bar -Scrumdiddlyumptious</td>\n",
       "      <td>7.20</td>\n",
       "      <td>2</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>Lot's O' Nuts</td>\n",
       "      <td>32.881893</td>\n",
       "      <td>-111.768036</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>138100</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>10011</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Wonka Bar - Fudge Mallows</td>\n",
       "      <td>7.20</td>\n",
       "      <td>2</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.40</td>\n",
       "      <td>Lot's O' Nuts</td>\n",
       "      <td>32.881893</td>\n",
       "      <td>-111.768036</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2022-03-13</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>138100</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>10011</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Wonka Bar - Milk Chocolate</td>\n",
       "      <td>9.75</td>\n",
       "      <td>3</td>\n",
       "      <td>6.33</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Wicked Choccy's</td>\n",
       "      <td>32.076176</td>\n",
       "      <td>-81.088371</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>First Class</td>\n",
       "      <td>121391</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>94109</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Wonka Bar - Milk Chocolate</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>Wicked Choccy's</td>\n",
       "      <td>32.076176</td>\n",
       "      <td>-81.088371</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order_Date  Ship_Date       Ship_Mode  Customer_ID Country/Region  \\\n",
       "0 2021-03-31 2026-09-26  Standard Class       128055  United States   \n",
       "1 2021-03-31 2026-09-26  Standard Class       128055  United States   \n",
       "2 2021-09-15 2022-03-13  Standard Class       138100  United States   \n",
       "3 2021-09-15 2022-03-13  Standard Class       138100  United States   \n",
       "4 2022-10-04 2023-03-29     First Class       121391  United States   \n",
       "\n",
       "            City State/Province Postal_Code   Division    Region  \\\n",
       "0  San Francisco     California       94122  Chocolate   Pacific   \n",
       "1  San Francisco     California       94122  Chocolate   Pacific   \n",
       "2  New York City       New York       10011  Chocolate  Atlantic   \n",
       "3  New York City       New York       10011  Chocolate  Atlantic   \n",
       "4  San Francisco     California       94109  Chocolate   Pacific   \n",
       "\n",
       "                        Product_Name  Sales  Units  Gross_Profit  Cost  \\\n",
       "0  Wonka Bar - Triple Dazzle Caramel   7.50      2          4.90  2.60   \n",
       "1     Wonka Bar -Scrumdiddlyumptious   7.20      2          5.00  2.20   \n",
       "2          Wonka Bar - Fudge Mallows   7.20      2          4.80  2.40   \n",
       "3         Wonka Bar - Milk Chocolate   9.75      3          6.33  3.42   \n",
       "4         Wonka Bar - Milk Chocolate   6.50      2          4.22  2.28   \n",
       "\n",
       "           Factory   Latitude   Longitude  ship_delay_days  \n",
       "0  Wicked Choccy's  32.076176  -81.088371             2005  \n",
       "1    Lot's O' Nuts  32.881893 -111.768036             2005  \n",
       "2    Lot's O' Nuts  32.881893 -111.768036              179  \n",
       "3  Wicked Choccy's  32.076176  -81.088371              179  \n",
       "4  Wicked Choccy's  32.076176  -81.088371              176  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show first rows of the existing DataFrame `df`\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8344fd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>City</th>\n",
       "      <th>State/Province</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Division</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Units</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Factory</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>128055</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>94122</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Wonka Bar - Triple Dazzle Caramel</td>\n",
       "      <td>7.50</td>\n",
       "      <td>2</td>\n",
       "      <td>4.90</td>\n",
       "      <td>2.60</td>\n",
       "      <td>Wicked Choccy's</td>\n",
       "      <td>32.076176</td>\n",
       "      <td>-81.088371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>128055</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>94122</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Wonka Bar -Scrumdiddlyumptious</td>\n",
       "      <td>7.20</td>\n",
       "      <td>2</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>Lot's O' Nuts</td>\n",
       "      <td>32.881893</td>\n",
       "      <td>-111.768036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2027-03-13</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>138100</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>10011</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Wonka Bar - Fudge Mallows</td>\n",
       "      <td>7.20</td>\n",
       "      <td>2</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.40</td>\n",
       "      <td>Lot's O' Nuts</td>\n",
       "      <td>32.881893</td>\n",
       "      <td>-111.768036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2027-03-13</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>138100</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>10011</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Wonka Bar - Milk Chocolate</td>\n",
       "      <td>9.75</td>\n",
       "      <td>3</td>\n",
       "      <td>6.33</td>\n",
       "      <td>3.42</td>\n",
       "      <td>Wicked Choccy's</td>\n",
       "      <td>32.076176</td>\n",
       "      <td>-81.088371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2028-03-29</td>\n",
       "      <td>First Class</td>\n",
       "      <td>121391</td>\n",
       "      <td>United States</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>94109</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Wonka Bar - Milk Chocolate</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>Wicked Choccy's</td>\n",
       "      <td>32.076176</td>\n",
       "      <td>-81.088371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Order Date   Ship Date       Ship Mode  Customer ID  \\\n",
       "0           0  2021-03-31  2026-09-26  Standard Class       128055   \n",
       "1           1  2021-03-31  2026-09-26  Standard Class       128055   \n",
       "2           2  2021-09-15  2027-03-13  Standard Class       138100   \n",
       "3           3  2021-09-15  2027-03-13  Standard Class       138100   \n",
       "4           4  2022-10-04  2028-03-29     First Class       121391   \n",
       "\n",
       "  Country/Region           City State/Province Postal Code   Division  \\\n",
       "0  United States  San Francisco     California       94122  Chocolate   \n",
       "1  United States  San Francisco     California       94122  Chocolate   \n",
       "2  United States  New York City       New York       10011  Chocolate   \n",
       "3  United States  New York City       New York       10011  Chocolate   \n",
       "4  United States  San Francisco     California       94109  Chocolate   \n",
       "\n",
       "     Region                       Product Name  Sales  Units  Gross Profit  \\\n",
       "0   Pacific  Wonka Bar - Triple Dazzle Caramel   7.50      2          4.90   \n",
       "1   Pacific     Wonka Bar -Scrumdiddlyumptious   7.20      2          5.00   \n",
       "2  Atlantic          Wonka Bar - Fudge Mallows   7.20      2          4.80   \n",
       "3  Atlantic         Wonka Bar - Milk Chocolate   9.75      3          6.33   \n",
       "4   Pacific         Wonka Bar - Milk Chocolate   6.50      2          4.22   \n",
       "\n",
       "   Cost          Factory   Latitude   Longitude  \n",
       "0  2.60  Wicked Choccy's  32.076176  -81.088371  \n",
       "1  2.20    Lot's O' Nuts  32.881893 -111.768036  \n",
       "2  2.40    Lot's O' Nuts  32.881893 -111.768036  \n",
       "3  3.42  Wicked Choccy's  32.076176  -81.088371  \n",
       "4  2.28  Wicked Choccy's  32.076176  -81.088371  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick look at Data columns and types\n",
    "df.columns\n",
    "df.dtypes.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe21865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10194 entries, 0 to 10193\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      10194 non-null  int64  \n",
      " 1   Order Date      10194 non-null  object \n",
      " 2   Ship Date       10194 non-null  object \n",
      " 3   Ship Mode       10194 non-null  object \n",
      " 4   Customer ID     10194 non-null  int64  \n",
      " 5   Country/Region  10194 non-null  object \n",
      " 6   City            10194 non-null  object \n",
      " 7   State/Province  10194 non-null  object \n",
      " 8   Postal Code     10194 non-null  object \n",
      " 9   Division        10194 non-null  object \n",
      " 10  Region          10194 non-null  object \n",
      " 11  Product Name    10194 non-null  object \n",
      " 12  Sales           10194 non-null  float64\n",
      " 13  Units           10194 non-null  int64  \n",
      " 14  Gross Profit    10194 non-null  float64\n",
      " 15  Cost            10194 non-null  float64\n",
      " 16  Factory         10194 non-null  object \n",
      " 17  Latitude        10194 non-null  float64\n",
      " 18  Longitude       10194 non-null  float64\n",
      "dtypes: float64(5), int64(3), object(11)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81eb99",
   "metadata": {},
   "source": [
    "# Data Cleaning and Filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a643c",
   "metadata": {},
   "source": [
    "# Clean and Inspect date columns (Order_Date , Ship_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d722c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Ship_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2027-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2027-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2028-03-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order_Date  Ship_Date\n",
       "0 2021-03-31 2026-09-26\n",
       "1 2021-03-31 2026-09-26\n",
       "2 2021-09-15 2027-03-13\n",
       "3 2021-09-15 2027-03-13\n",
       "4 2022-10-04 2028-03-29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize column names to snake_case (e.g. \"Order Date\" -> \"Order_Date\")\n",
    "df = df.rename(columns=lambda s: s.strip().replace(' ', '_'))\n",
    "\n",
    "# convert columns to datetime (use .get for safety)\n",
    "df[\"Order_Date\"] = pd.to_datetime(df.get(\"Order_Date\"), errors=\"coerce\")\n",
    "df[\"Ship_Date\"] = pd.to_datetime(df.get(\"Ship_Date\"), errors=\"coerce\")\n",
    "\n",
    "# Optionally drop rows where order date is missing\n",
    "df = df.dropna(subset=[\"Order_Date\"])\n",
    "df[[\"Order_Date\", \"Ship_Date\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ed7c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Ship_Date</th>\n",
       "      <th>ship_delay_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2026-09-26</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2027-03-13</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>2027-03-13</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2028-03-29</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order_Date  Ship_Date  ship_delay_days\n",
       "0 2021-03-31 2026-09-26             2005\n",
       "1 2021-03-31 2026-09-26             2005\n",
       "2 2021-09-15 2027-03-13             2005\n",
       "3 2021-09-15 2027-03-13             2005\n",
       "4 2022-10-04 2028-03-29             2003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# already converted to datetime\n",
    "df[\"Order_Date\"] = pd.to_datetime(df[\"Order_Date\"], errors=\"coerce\")\n",
    "df[\"Ship_Date\"] = pd.to_datetime(df[\"Ship_Date\"], errors=\"coerce\")\n",
    "\n",
    "# compute shipping delay in days\n",
    "df[\"ship_delay_days\"] = (df[\"Ship_Date\"] - df[\"Order_Date\"]).dt.days\n",
    "\n",
    "# inspect extreme delays\n",
    "df[df[\"ship_delay_days\"] > 60][[\"Order_Date\", \"Ship_Date\", \"ship_delay_days\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ec1d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10194.000000\n",
      "mean       305.072101\n",
      "std        465.226740\n",
      "min        173.000000\n",
      "25%        177.000000\n",
      "50%        178.000000\n",
      "75%        179.000000\n",
      "max       2007.000000\n",
      "Name: ship_delay_days, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# --- 1. Identify Column Names ---\n",
    "if \"Order_Date\" in df.columns:\n",
    "\tod_col = \"Order_Date\"\n",
    "elif \"Order Date\" in df.columns:\n",
    "\tod_col = \"Order Date\"\n",
    "else:\n",
    "\traise KeyError(\"Order date column not found in dataframe\")\n",
    "\n",
    "# Set the Ship Date column name (assuming 'Ship_Date' is the normalized one)\n",
    "if \"Ship_Date\" in df.columns:\n",
    "\tsd_col = \"Ship_Date\"\n",
    "elif \"Ship Date\" in df.columns:\n",
    "\tsd_col = \"Ship Date\"\n",
    "else:\n",
    "    # If this column is missing, the following steps will fail, \n",
    "    # but based on your description, it is present, just with wrong years.\n",
    "\tsd_col = \"Ship_Date\" \n",
    "\n",
    "# --- 2. Ensure Both Columns are Datetime Objects ---\n",
    "df[od_col] = pd.to_datetime(df[od_col], errors=\"coerce\")\n",
    "# The original Ship_Date data is still present and converted here!\n",
    "df[sd_col] = pd.to_datetime(df[sd_col], errors=\"coerce\") \n",
    "\n",
    "# --- 3. APPLY THE 5-YEAR CORRECTION (The crucial step) ---\n",
    "# Check where the Ship_Date year is systematically wrong (e.g., 2027)\n",
    "mask_erroneous_year = df[sd_col].dt.year >= 2027 # Check for 2027 or later\n",
    "\n",
    "# Subtract 5 years ONLY from the rows where the year is wrong\n",
    "df.loc[mask_erroneous_year, sd_col] = \\\n",
    "    df.loc[mask_erroneous_year, sd_col] - DateOffset(years=5)\n",
    "\n",
    "# --- 4. Compute Delay and Show Summary ---\n",
    "df[\"ship_delay_days\"] = (df[sd_col] - df[od_col]).dt.days\n",
    "print(df[\"ship_delay_days\"].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8380962",
   "metadata": {},
   "source": [
    "# Check for Missing Values across all Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9e66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed:_0         0\n",
      "Order_Date         0\n",
      "Ship_Date          0\n",
      "Ship_Mode          0\n",
      "Customer_ID        0\n",
      "Country/Region     0\n",
      "City               0\n",
      "State/Province     0\n",
      "Postal_Code        0\n",
      "Division           0\n",
      "Region             0\n",
      "Product_Name       0\n",
      "Sales              0\n",
      "Units              0\n",
      "Gross_Profit       0\n",
      "Cost               0\n",
      "Factory            0\n",
      "Latitude           0\n",
      "Longitude          0\n",
      "ship_delay_days    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values across all columns\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd20a8e",
   "metadata": {},
   "source": [
    "# Check for Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad5ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of exact duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for any exact duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "\n",
    "print(f\"Total number of exact duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"\\nDisplaying the first 5 duplicated rows (keeping the first occurrence):\")\n",
    "    # Display the rows that are duplicates (where df.duplicated() is True)\n",
    "    print(df[df.duplicated()].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b72ff",
   "metadata": {},
   "source": [
    "# Check for Unique Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be69cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ship_Mode unique values ---\n",
      "Ship_Mode\n",
      "Standard Class    6120\n",
      "Second Class      1979\n",
      "First Class       1548\n",
      "Same Day           547\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Division unique values ---\n",
      "Division\n",
      "Chocolate    9844\n",
      "Other         310\n",
      "Sugar          40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Region unique values ---\n",
      "Region\n",
      "Pacific     3253\n",
      "Atlantic    2986\n",
      "Interior    2335\n",
      "Gulf        1620\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Factory unique values ---\n",
      "Factory\n",
      "Lot's O' Nuts        5692\n",
      "Wicked Choccy's      4152\n",
      "Secret Factory        217\n",
      "The Other Factory     100\n",
      "Sugar Shack            33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values for key categorical columns\n",
    "\n",
    "print(\"--- Ship_Mode unique values ---\")\n",
    "print(df['Ship_Mode'].value_counts())\n",
    "\n",
    "print(\"\\n--- Division unique values ---\")\n",
    "print(df['Division'].value_counts())\n",
    "\n",
    "print(\"\\n--- Region unique values ---\")\n",
    "print(df['Region'].value_counts())\n",
    "\n",
    "print(\"\\n--- Factory unique values ---\")\n",
    "print(df['Factory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bd4a6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Division\n",
       "Chocolate    96.6\n",
       "Other         3.0\n",
       "Sugar         0.4\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shipping delay by ship mode\n",
    "df.groupby(\"Ship_Mode\")[\"ship_delay_days\"].mean().sort_values()\n",
    "\n",
    "# Orders by factory and region\n",
    "pd.crosstab(df[\"Factory\"], df[\"Region\"])\n",
    "\n",
    "# Division share\n",
    "(df[\"Division\"].value_counts(normalize=True) * 100).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc4d80",
   "metadata": {},
   "source": [
    "# Checking Geographical Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5a13a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Country/Region unique values ---\n",
      "Country/Region\n",
      "United States    9994\n",
      "Canada            200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- State/Province unique values (Top 10) ---\n",
      "State/Province\n",
      "California        2001\n",
      "New York          1128\n",
      "Texas              985\n",
      "Pennsylvania       587\n",
      "Washington         506\n",
      "Illinois           492\n",
      "Ohio               469\n",
      "Florida            383\n",
      "Michigan           255\n",
      "North Carolina     249\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check unique values for remaining categorical columns\n",
    "\n",
    "print(\"--- Country/Region unique values ---\")\n",
    "print(df['Country/Region'].value_counts())\n",
    "\n",
    "print(\"\\n--- State/Province unique values (Top 10) ---\")\n",
    "# Only show the top 10 as there might be many states/provinces\n",
    "print(df['State/Province'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e4171",
   "metadata": {},
   "source": [
    "# The Column Unnamed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34a1db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 'Unnamed:_0' column.\n",
      "\n",
      "Final Data Types Check:\n",
      "Order_Date         datetime64[ns]\n",
      "Ship_Date          datetime64[ns]\n",
      "Ship_Mode                  object\n",
      "Customer_ID                 int64\n",
      "Country/Region             object\n",
      "City                       object\n",
      "State/Province             object\n",
      "Postal_Code                object\n",
      "Division                   object\n",
      "Region                     object\n",
      "Product_Name               object\n",
      "Sales                     float64\n",
      "Units                       int64\n",
      "Gross_Profit              float64\n",
      "Cost                      float64\n",
      "Factory                    object\n",
      "Latitude                  float64\n",
      "Longitude                 float64\n",
      "ship_delay_days             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Final Step: Drop the redundant index column\n",
    "if 'Unnamed:_0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed:_0'])\n",
    "    print(\"Dropped 'Unnamed:_0' column.\")\n",
    "\n",
    "# Quick check of the final data types\n",
    "print(\"\\nFinal Data Types Check:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3bfba",
   "metadata": {},
   "source": [
    "# Prepare and Create factory.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c7aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Geography Created ---\n",
      "File: Geography.csv | Rows: 1386\n",
      "   Geo_Key Country/Region           City State/Province Postal_Code    Region  \\\n",
      "0        1  United States  San Francisco     California       94122   Pacific   \n",
      "1        2  United States  San Francisco     California       94122   Pacific   \n",
      "2        3  United States  New York City       New York       10011  Atlantic   \n",
      "3        4  United States  New York City       New York       10011  Atlantic   \n",
      "4        5  United States  San Francisco     California       94109   Pacific   \n",
      "\n",
      "           Factory   Latitude   Longitude  \n",
      "0  Wicked Choccy's  32.076176  -81.088371  \n",
      "1    Lot's O' Nuts  32.881893 -111.768036  \n",
      "2    Lot's O' Nuts  32.881893 -111.768036  \n",
      "3  Wicked Choccy's  32.076176  -81.088371  \n",
      "4  Wicked Choccy's  32.076176  -81.088371  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your clean DataFrame\n",
    "\n",
    "# Columns that define the unique geographic entity\n",
    "geo_cols = [\n",
    "    'Country/Region', 'City', 'State/Province', 'Postal_Code',\n",
    "    'Region', 'Factory', 'Latitude', 'Longitude'\n",
    "]\n",
    "\n",
    "# 1. Create the Dimension table by taking only unique combinations\n",
    "geography = df[geo_cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 2. Add the Surrogate Key (Primary Key)\n",
    "# This key will link the Fact table to this Dimension table.\n",
    "geography.index.name = 'Geo_Key'\n",
    "geography = geography.reset_index()\n",
    "geography['Geo_Key'] = geography['Geo_Key'] + 1 \n",
    "\n",
    "# 3. Export to CSV\n",
    "# use the actual dataframe variable 'geography' (was incorrectly referenced as dim_geography)\n",
    "geography.to_csv('Geography.csv', index=False)\n",
    "\n",
    "print(\"--- Geography Created ---\")\n",
    "print(f\"File: Geography.csv | Rows: {len(geography)}\")\n",
    "print(geography.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75039ca2",
   "metadata": {},
   "source": [
    "# Orders.csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93727f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Orders Created ---\n",
      "File: Orders.csv | Rows: 10194\n",
      "   Customer_ID Order_date  Ship_date       Ship_mode\n",
      "0       128055 2021-03-31 2026-09-26  Standard Class\n",
      "1       128055 2021-03-31 2026-09-26  Standard Class\n",
      "2       138100 2021-09-15 2022-03-13  Standard Class\n",
      "3       138100 2021-09-15 2022-03-13  Standard Class\n",
      "4       121391 2022-10-04 2023-03-29     First Class\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your clean DataFrame\n",
    "# and 'dim_geography' was created in the previous step\n",
    "\n",
    "# Define the columns that were used to find the unique geography (use existing geo_cols if available)\n",
    "cols = [\n",
    "    'Country/Region', 'City', 'State/Province', 'Postal_Code',\n",
    "    'Region', 'Factory', 'Latitude', 'Longitude'\n",
    "]\n",
    "\n",
    "# --- Safe merge: only use the geography columns that exist in both dataframes ---\n",
    "merge_cols = [c for c in geo_cols if c in df.columns and c in geography.columns]\n",
    "if not merge_cols:\n",
    "    raise KeyError(\"No common geography columns found between df and geography\")\n",
    "\n",
    "# 1. Merge the new Geo_Key back into the main DataFrame\n",
    "# This adds the Geo_Key (Foreign Key) to every order row.\n",
    "fact_orders = pd.merge(\n",
    "    df,\n",
    "    geography[['Geo_Key'] + merge_cols],  # Merge on Geo_Key and the lookup columns\n",
    "    on=merge_cols,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 2. Select only the necessary Fact table columns (metrics + keys)\n",
    "# include Geo_Key and common metrics/attributes you expect\n",
    "fact_cols = [\n",
    "    'Customer_ID',\n",
    "    \n",
    "    'Order_Date',\n",
    "    'Ship_Date',\n",
    "    'Ship_Mode',\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "# Keep only the columns that actually exist to avoid KeyError\n",
    "fact_cols_existing = [c for c in fact_cols if c in fact_orders.columns]\n",
    "orders = fact_orders[fact_cols_existing].copy()\n",
    "\n",
    "# Optional: rename columns to match downstream expectations (safe rename)\n",
    "rename_map_local = {\n",
    "    'Order_Date': 'Order_date',\n",
    "    'Ship_Date': 'Ship_date',\n",
    "    'Ship_Mode': 'Ship_mode',\n",
    "    'Gross_Profit': 'Profit'  # if you want Gross_Profit renamed to Profit downstream\n",
    "}\n",
    "\n",
    "# only keep mappings for columns that exist in orders\n",
    "rename_map_local = {k: v for k, v in rename_map_local.items() if k in orders.columns}\n",
    "orders = orders.rename(columns=rename_map_local)\n",
    "\n",
    "# 3. Export to CSV\n",
    "orders.to_csv('Orders.csv', index=False)\n",
    "\n",
    "print(\"--- Orders Created ---\")\n",
    "print(f\"File: Orders.csv | Rows: {len(orders)}\")\n",
    "print(orders.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61c1259",
   "metadata": {},
   "source": [
    "# Product.csv and ShipMode.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8daa7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders columns: ['Customer_ID', 'Order_Date', 'Ship_Date', 'Ship_Mode']\n",
      "fact_orders columns: ['Order_Date', 'Ship_Date', 'Ship_Mode', 'Customer_ID', 'Country/Region', 'City', 'State/Province', 'Postal_Code', 'Division', 'Region', 'Product_Name', 'Sales', 'Units', 'Gross_Profit', 'Cost', 'Factory', 'Latitude', 'Longitude', 'ship_delay_days', 'Geo_Key', 'Product_ID_x', 'Ship_Mode_ID_x', 'Product_ID_y', 'Ship_Mode_ID_y', 'Ship_Mode_ID', 'Product_ID']\n"
     ]
    }
   ],
   "source": [
    "print(\"orders columns:\", orders.columns.tolist())\n",
    "print(\"fact_orders columns:\", fact_orders.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b23ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standardise column names\n",
    "orders = orders.rename(columns={\n",
    "    'Order_date': 'Order_Date',\n",
    "    'Ship_date':  'Ship_Date',\n",
    "    'Ship_mode':  'Ship_Mode'\n",
    "})\n",
    "\n",
    "# fact_orders already has the right names, but do it defensively\n",
    "fact_orders = fact_orders.rename(columns={\n",
    "    'Order_date': 'Order_Date',\n",
    "    'Ship_date':  'Ship_Date',\n",
    "    'Ship_mode':  'Ship_Mode'\n",
    "})\n",
    "\n",
    "# 2. Build ShipMode dimension safely to avoid merge-suffix collisions\n",
    "ship_mode_cols = ['Ship_Mode']\n",
    "\n",
    "# If a Ship_Mode ID mapping already exists in fact_orders (possibly from earlier merges),\n",
    "# reuse it instead of creating & merging again (which causes duplicate column name errors).\n",
    "existing_id_col = None\n",
    "if 'Ship_Mode_ID' in fact_orders.columns:\n",
    "    existing_id_col = 'Ship_Mode_ID'\n",
    "else:\n",
    "    # look for any variant created by earlier merges (e.g. Ship_Mode_ID_x or Ship_Mode_ID_y)\n",
    "    candidates = [c for c in fact_orders.columns if c.startswith('Ship_Mode_ID')]\n",
    "    if candidates:\n",
    "        existing_id_col = candidates[0]\n",
    "\n",
    "if existing_id_col:\n",
    "    # Reconstruct a clean Ship Mode dimension from existing mapping\n",
    "    ship_mode = (\n",
    "        fact_orders[[ 'Ship_Mode', existing_id_col ]]\n",
    "        .drop_duplicates()\n",
    "        .rename(columns={existing_id_col: 'Ship_Mode_ID'})\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    # Create new Ship Mode dimension and merge once\n",
    "    ship_mode = fact_orders[ship_mode_cols].drop_duplicates().reset_index(drop=True)\n",
    "    ship_mode.index.name = 'Ship_Mode_ID'\n",
    "    ship_mode = ship_mode.reset_index()\n",
    "    ship_mode['Ship_Mode_ID'] = ship_mode['Ship_Mode_ID'] + 1\n",
    "\n",
    "    # Merge only when we created new IDs (avoids creating duplicate suffixed columns)\n",
    "    fact_orders = pd.merge(fact_orders, ship_mode, on=ship_mode_cols, how='left')\n",
    "\n",
    "# Ensure consistent ordering: Ship_Mode_ID then Ship_Mode\n",
    "if 'Ship_Mode_ID' in ship_mode.columns:\n",
    "    cols_order = ['Ship_Mode_ID'] + [c for c in ship_mode_cols if c in ship_mode.columns]\n",
    "    ship_mode = ship_mode[cols_order]\n",
    "\n",
    "ship_mode.to_csv('Dim_ShipMode.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1469a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Product and ShipMode Created ---\n",
      "Product Rows: 15\n",
      "ShipMode Rows: 4\n",
      "\n",
      "First 5 rows of Product:\n",
      "                        Product_Name   Division  Product_ID\n",
      "0  Wonka Bar - Triple Dazzle Caramel  Chocolate           1\n",
      "1     Wonka Bar -Scrumdiddlyumptious  Chocolate           2\n",
      "2          Wonka Bar - Fudge Mallows  Chocolate           3\n",
      "3         Wonka Bar - Milk Chocolate  Chocolate           4\n",
      "4  Wonka Bar - Nutty Crunch Surprise  Chocolate           5\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'fact_orders' is the DataFrame from the previous step (with Geo_Key)\n",
    "\n",
    "# --- A. Product ---\n",
    "product_cols = ['Product_Name', 'Division']\n",
    "\n",
    "# 1. Find a dataframe that contains the product columns (prefer fact_orders, then orders, then df)\n",
    "product_source = None\n",
    "for name in (\"fact_orders\", \"orders\", \"df\"):\n",
    "\tif name in globals():\n",
    "\t\tcand = globals()[name]\n",
    "\t\tif all(c in cand.columns for c in product_cols):\n",
    "\t\t\tproduct_source = cand\n",
    "\t\t\tproduct_source_name = name\n",
    "\t\t\tbreak\n",
    "\n",
    "if product_source is None:\n",
    "\traise KeyError(\"None of the dataframes (fact_orders, orders, df) contain the required product columns: \"\n",
    "\t\t\t\t   f\"{product_cols!r}\")\n",
    "\n",
    "# 2. Create the unique dimension table from the discovered source\n",
    "product = product_source[product_cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 3. Add the Surrogate Key (Product_ID)\n",
    "product.index.name = 'Product_ID'\n",
    "product = product.reset_index()\n",
    "product['Product_ID'] = product['Product_ID'] + 1 \n",
    "\n",
    "# 4. Merge back the Product_ID (Foreign Key) to the main table\n",
    "# Prefer merging into existing fact_orders; if not present, merge into orders and assign to fact_orders\n",
    "merge_target = globals().get('fact_orders', globals().get('orders'))\n",
    "\n",
    "# operate on a local reference to avoid accidental overwrites\n",
    "fact_orders_ref = merge_target\n",
    "\n",
    "# If Product_ID already exists in the target, avoid re-merging to prevent suffix/duplicate errors.\n",
    "if 'Product_ID' in fact_orders_ref.columns:\n",
    "\t# Reconstruct a consistent Product dimension from the existing mapping\n",
    "\tproduct = fact_orders_ref[product_cols + ['Product_ID']].drop_duplicates().reset_index(drop=True)\n",
    "\t# keep the existing main table as fact_orders for downstream steps\n",
    "\tfact_orders = fact_orders_ref\n",
    "else:\n",
    "\t# Safe merge when Product_ID does not already exist in fact_orders\n",
    "\tfact_orders = pd.merge(fact_orders_ref, product, on=product_cols, how='left')\n",
    "\n",
    "product.to_csv('Product.csv', index=False)\n",
    "\n",
    "\n",
    "# --- B. Dim_ShipMode ---\n",
    "# Safely detect the correct Ship Mode column name (some earlier cells renamed it to 'Ship_mode')\n",
    "if 'Ship_Mode' in orders.columns:\n",
    "\tsm_col = 'Ship_Mode'\n",
    "elif 'Ship_mode' in orders.columns:\n",
    "\tsm_col = 'Ship_mode'\n",
    "else:\n",
    "\traise KeyError(\"Ship mode column not found in 'orders' dataframe (expected 'Ship_Mode' or 'Ship_mode')\")\n",
    "\n",
    "ship_mode_cols = [sm_col]\n",
    "\n",
    "# 1. Create the unique dimension table from orders\n",
    "ship_mode = orders[ship_mode_cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 2. Add the Surrogate Key (Ship_Mode_ID)\n",
    "ship_mode.index.name = 'Ship_Mode_ID'\n",
    "ship_mode = ship_mode.reset_index()\n",
    "ship_mode['Ship_Mode_ID'] = ship_mode['Ship_Mode_ID'] + 1 \n",
    "\n",
    "# 3. Merge back the Ship_Mode_ID (Foreign Key) to the main table\n",
    "# Avoid re-merging if fact_orders already contains a Ship_Mode_ID column to prevent merge suffix collisions.\n",
    "if 'Ship_Mode_ID' in fact_orders.columns:\n",
    "\t# Reconstruct a consistent ShipMode dimension from the existing mapping in fact_orders\n",
    "\t# This avoids merge conflicts while still exporting a correct Dim_ShipMode.csv\n",
    "\tship_mode = fact_orders[['Ship_Mode', 'Ship_Mode_ID']].drop_duplicates().reset_index(drop=True)\n",
    "else:\n",
    "\t# Safe merge when Ship_Mode_ID does not already exist in fact_orders\n",
    "\tfact_orders = pd.merge(fact_orders, ship_mode, on=ship_mode_cols, how='left')\n",
    "\n",
    "# Ensure column order is consistent: Ship_Mode_ID then Ship_Mode\n",
    "if 'Ship_Mode_ID' in ship_mode.columns:\n",
    "\tcols_order = ['Ship_Mode_ID', sm_col] if sm_col in ship_mode.columns else ['Ship_Mode_ID']\n",
    "\tship_mode = ship_mode[cols_order]\n",
    "\n",
    "ship_mode.to_csv('Dim_ShipMode.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"--- Product and ShipMode Created ---\")\n",
    "print(f\"Product Rows: {len(product)}\")\n",
    "print(f\"ShipMode Rows: {len(ship_mode)}\")\n",
    "print(\"\\nFirst 5 rows of Product:\")\n",
    "print(product.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51bdfc",
   "metadata": {},
   "source": [
    "# Sales.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5650d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Sales_Item (Bridge) Created ---\n",
      "Sales_Item Rows: 10194\n",
      "\n",
      "First 5 rows of Sales_Item:\n",
      "   Order_Line_Key  Order_ID\n",
      "0               1         1\n",
      "1               2         2\n",
      "2               3         3\n",
      "3               4         4\n",
      "4               5         5\n"
     ]
    }
   ],
   "source": [
    "# Create Sales_Item (bridge) from the available merged dataframe\n",
    "# Prefer 'orders' if present (it already contains keys), otherwise use 'orders'\n",
    "src = orders if 'orders' in globals() else orders\n",
    "\n",
    "# Ensure Order_ID exists\n",
    "if 'Order_ID' not in src.columns:\n",
    "    src = src.reset_index(drop=True).copy()\n",
    "    src['Order_ID'] = src.index + 1\n",
    "\n",
    "# Ensure Product_ID exists (try to merge from 'product' dim if possible)\n",
    "if 'Product_ID' not in src.columns:\n",
    "    if 'Product_ID_x' in src.columns:\n",
    "        src['Product_ID'] = src['Product_ID_x']\n",
    "    elif 'Product_ID_y' in src.columns:\n",
    "        src['Product_ID'] = src['Product_ID_y']\n",
    "    else:\n",
    "        if 'Product_Name' in src.columns and 'product' in globals():\n",
    "            src = src.merge(product[['Product_ID', 'Product_Name']], on='Product_Name', how='left')\n",
    "        # If still missing, create synthetic Product_ID per unique product name (stable)\n",
    "        if 'Product_ID' not in src.columns and 'Product_Name' in src.columns:\n",
    "            src['Product_ID'] = src['Product_Name'].factorize()[0] + 1\n",
    "\n",
    "# Determine which profit column exists\n",
    "profit_col = None\n",
    "if 'Gross_Profit' in src.columns:\n",
    "    profit_col = 'Gross_Profit'\n",
    "elif 'Profit' in src.columns:\n",
    "    profit_col = 'Profit'\n",
    "\n",
    "# Build the list of columns to export (only include those that exist)\n",
    "sales_cols = ['Order_ID', 'Product_ID', 'Sales', 'Units', 'Cost']\n",
    "if profit_col:\n",
    "    sales_cols.insert(4, profit_col)  # place profit column before Cost\n",
    "\n",
    "cols_to_select = [c for c in sales_cols if c in src.columns]\n",
    "\n",
    "# Select and build the fact table\n",
    "sales_item = src[cols_to_select].copy()\n",
    "\n",
    "# If profit column is named 'Profit' but we want 'Gross_Profit' in output, rename it\n",
    "if profit_col == 'Profit':\n",
    "    sales_item = sales_item.rename(columns={'Profit': 'Gross_Profit'})\n",
    "\n",
    "# Add a Primary Key for the line item\n",
    "sales_item.index.name = 'Order_Line_Key'\n",
    "sales_item = sales_item.reset_index()\n",
    "sales_item['Order_Line_Key'] = sales_item['Order_Line_Key'] + 1\n",
    "\n",
    "# Export to CSV\n",
    "sales_item.to_csv('Sales_Item.csv', index=False)\n",
    "\n",
    "print(\"---Sales_Item (Bridge) Created ---\")\n",
    "print(f\"Sales_Item Rows: {len(sales_item)}\")\n",
    "print(\"\\nFirst 5 rows of Sales_Item:\")\n",
    "print(sales_item.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff4bbc",
   "metadata": {},
   "source": [
    "# customer.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0d60cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Customer Created ---\n",
      "File: Customer.csv | Unique Customer Rows: 5044\n",
      "   Customer_ID\n",
      "0       128055\n",
      "1       138100\n",
      "2       121391\n",
      "3       103982\n",
      "4       147039\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your clean DataFrame, we just need the unique Customer IDs\n",
    "customer_cols = ['Customer_ID']\n",
    "\n",
    "# 1. Create the unique dimension table\n",
    "customer = df[customer_cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 2. Export to CSV\n",
    "customer.to_csv('Dim_Customer.csv', index=False)\n",
    "\n",
    "print(\"--- Customer Created ---\")\n",
    "print(f\"File: Customer.csv | Unique Customer Rows: {len(customer)}\")\n",
    "print(customer.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
